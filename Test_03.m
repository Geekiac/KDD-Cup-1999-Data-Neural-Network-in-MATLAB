% Test_03.m
% Results_03.mat - Workspace variables generated by this experiment
%
% Variable parameters settings compared using this test:
%
% hidden layer transfer functions = only 7 possible functions
% normalizing functions - mapminmax and mapstd
% Principal component analysis = on or off
%
% Fixed parameters for this test:
%
% target dataset size for experiment = 5000 (May be less due to rounding)
% network topology - 3 hidden layers containing 5 neurons each
% data split percentages train/validate/test = 10/90/0
% number of epochs = 100
% number of folds for random subsampling = 10
%
% Fixed parameters for all tests:
%
% 'patternnet' is used to generate the networks.
% training function = 'trainscg' - Scaled conjugate gradient backpropagation
% performance function = 'crossentropy'
% output function = 'softmax' - Soft max transfer function
% random sampling is used to divide data for training and validation

tstart_entire_test = tic % for timing the execution of the experiment

% load transformed 10 percent of kddcup 99 data (494021 rows)
load assignment2_data.mat kddcup_array result_array

% transpose arrays for use with the neural network
x = kddcup_array'; % contains 41 dimensions
t = result_array'; % contains the corresponding classification

% remove original arrays - to free up memory
clear kddcup_array result_array

% -- Create smaller stratified dataset for testing
newT = [];
newX = [];

total = size(t,2);
% target dataset size for testing (May be less due to rounding)
target = 5000;
totalBinSize = 0;
for i=1:23
	% get indices of relating to a classification
    idx = find(vec2ind(t(:,:)) == i);
    idxSize = length(idx);
	
	% determine how large each classication should be
	% related to the original dataset
    binSize = round((idxSize * target) / total);
    totalBinSize = totalBinSize + binSize;
    
	% indices of classification i for the determined binSize
    idx = idx(1:min(idxSize,binSize));
	
	% add classification subsets to new datasets
    newT = [newT t(:, idx)];
    newX = [newX x(:, idx)];
end

% -- stratified datasets replaces the original datasets
x = newX;
t = newT;

TransferFcn = ...
{ 'elliotsig'; ... % Elliot sigmoid transfer function.
  'logsig'; ...    % Logarithmic sigmoid transfer function.
  'purelin'; ...   % Linear transfer function.
  'radbas'; ...    % Radial basis transfer function.
  'satlin'; ...    % Positive saturating linear transfer function.
  'satlins'; ...   % Symmetric saturating linear transfer function.
  'tansig';}    % Symmetric sigmoid transfer function.

outputTransferFcn = 'softmax'; % Fixed activation function for generating output.
NormalizingFcn = {'mapminmax','mapstd'}; % data normalizing functions

divideFcn = 'dividerand';  % Divide data randomly
divideMode = 'sample';  % Divide up every sample
trainRatio = 10/100; % 10 percent for training
valRatio = 90/100; % 90 percent for validation
testRatio = 0; % all of the data is used for either training or validation
epochs = 100; % a maximum of 100 epochs will be allowed for training

numTransferFcn = length(TransferFcn);
results = []; % This will be a table containing results data

k=10; % number of folds for random subsampling cross validation

hiddenLayerSize = [5 5 5];

for pca=0:1 % loop around Principal component analysis on or off
    for j=1:length(NormalizingFcn) % loop around data normalizing functions
        for i=1:length(TransferFcn) % loop around hidden layer activation functions
            tstart = tic;
            hiddenTransferFcn = TransferFcn{i};
            normalizeFcn = NormalizingFcn{j};
            inputProcessFunctions = {'removeconstantrows', normalizeFcn};
            if pca == 1
                inputProcessFunctions{3} = 'processpca';
            end
            outputProcessFunctions = {'removeconstantrows', normalizeFcn};

            % Create a Pattern Recognition Network
            net = GenerateKddCupNN(epochs, ...
                                    hiddenLayerSize, ...
                                    hiddenTransferFcn, ...
                                    outputTransferFcn, ...
                                    inputProcessFunctions, ...
                                    outputProcessFunctions, ...
                                    divideFcn, ...
                                    divideMode, ...
                                    trainRatio, ...
                                    valRatio, ...
                                    testRatio);


            % number of folds for random subsampling cross validation
            for foldIdx=1:k
                % Train the Network
                [net,tr] = train(net,x,t);

                % Test the Network
            	y = net(x); % NN generates classications from samples
				% calculate Cross Entropy between generated and actual results
                performance = perform(net,t,y);
			
				% calulate the percentage error 
				% i.e. the percentage of misclassified samples
                tind = vec2ind(t);
                yind = vec2ind(y);
                percentErrors = sum(tind ~= yind)/numel(tind);

            	% Calculate Training, Validation and Test Performance
                trainTargets = t .* tr.trainMask{1};
                valTargets = t .* tr.valMask{1};
                trainPerformance = perform(net,trainTargets,y);
                valPerformance = perform(net,valTargets,y);
                elapsedSeconds = toc(tstart); % time elapsed for this combination
                numEpochs = tr.num_epochs; % actual no. of epochs trained for

            	% data to put into the results table for each combination and fold 
                s = struct('HiddenTransferFcn', hiddenTransferFcn, ...
                            'HiddenLayerSize', hiddenLayerSize, ...
                            'NormalizingFcn', normalizeFcn, ...
                            'PCA', pca, ...
                            'NumEpochs', numEpochs, ...
                            'Fold', foldIdx, ...
                            'Performance', performance, ...
                            'PercentErrors', percentErrors, ...
                            'TrainPerformance', trainPerformance, ...
                            'ValPerformance', valPerformance, ...
                            'ElapsedSeconds', elapsedSeconds)

                results = [results; struct2table(s, 'AsArray', true)];
            end
        end
    end
end

% simplify results to the last result (Fold = 10)
% for each combination to compare actual NN percentage 
% error and performance
results_fold_10 = results(results.Fold == 10, :);
results_fold_10 = sortrows(results_fold_10,{'PercentErrors', 'Performance'},{'ascend', 'ascend'});
time_to_execute_entire_test = toc(tstart_entire_test)
% Test_13.m
% Results_13.mat - Workspace variables generated by this experiment
%
% This script assumes these variables are defined:
%
% network topology - 5 different hidden layer combinations
%
% Fixed parameters for this test:
%
% hidden layer transfer function = "radbas" Radial basis transfer function
% target dataset size for experiment = 494021 (full dataset)
% data split percentages train/validate/test = 30/70/0
% number of epochs = 100
% maximum number of folds for random subsampling = 100
% repeated subsampling until validation performance worse than previous fold
% data normalizing functions = mapminmax
% NO principal component analysis performed
%
% Fixed parameters for all tests:
%
% 'patternnet' is used to generate the networks.
% training function = 'trainscg' - Scaled conjugate gradient backpropagation
% performance function = 'crossentropy'
% output function = 'softmax' - Soft max transfer function
% random sampling is used to divide data for training and validation

tstart_entire_test = tic % for timing the execution of the experiment

% load transformed 10 percent of kddcup 99 data (494021 rows)
load assignment2_data.mat kddcup_array result_array

% transpose arrays for use with the neural network
x = kddcup_array'; % contains 41 dimensions
t = result_array'; % contains the corresponding classification

% remove original arrays - to free up memory
clear kddcup_array result_array

TransferFcn = { 'radbas' };   % Radial basis transfer function.

outputTransferFcn = 'softmax'; % Fixed activation function for generating output.
NormalizingFcn = {'mapminmax'}; % data normalizing functions

divideFcn = 'dividerand';  % Divide data randomly
divideMode = 'sample';  % Divide up every sample
trainRatio = 30/100; % 30 percent for training
valRatio = 70/100; % 70 percent for validation
testRatio = 0; % all of the data is used for either training or validation
epochs = 100; % a maximum of 100 epochs will be allowed for training

numTransferFcn = length(TransferFcn);
results = []; % This will be a table containing results data

k=100; % maximum number of folds for random subsampling cross validation

hiddenLayerSizes = {[35 35]; [20 20 20 20]; [20 20 20]; [15 15 15 15]; 20};
nets = []; % Holds all of the NNs generated by this experiment

for hls = 1:length(hiddenLayerSizes) % loop around hidden layer sizes
    for j=1:length(NormalizingFcn) % loop around data normalizing functions
        for i=1:length(TransferFcn) % loop around hidden layer activation functions
            oldValPerformance = 9999999; % initialise previous validation performance
            tstart = tic;
            hiddenLayerSize = hiddenLayerSizes{hls};
            hiddenTransferFcn = TransferFcn{i};
            normalizeFcn = NormalizingFcn{j};
            inputProcessFunctions = {'removeconstantrows', normalizeFcn};
            outputProcessFunctions = {'removeconstantrows', normalizeFcn};

            % Create a Pattern Recognition Network
            net = GenerateKddCupNN(epochs, ...
                                    hiddenLayerSize, ...
                                    hiddenTransferFcn, ...
                                    outputTransferFcn, ...
                                    inputProcessFunctions, ...
                                    outputProcessFunctions, ...
                                    divideFcn, ...
                                    divideMode, ...
                                    trainRatio, ...
                                    valRatio, ...
                                    testRatio);


            % number of folds for random subsampling cross validation
            for foldIdx=1:k
                % Train the Network
                [net,tr] = train(net,x,t);

                % Test the Network
            	y = net(x); % NN generates classications from samples
				% calculate Cross Entropy between generated and actual results
                performance = perform(net,t,y);
			
				% calulate the percentage error 
				% i.e. the percentage of misclassified samples
                tind = vec2ind(t);
                yind = vec2ind(y);
                percentErrors = sum(tind ~= yind)/numel(tind);

            	% Calculate Training, Validation and Test Performance
                trainTargets = t .* tr.trainMask{1};
                valTargets = t .* tr.valMask{1};
                trainPerformance = perform(net,trainTargets,y);
                valPerformance = perform(net,valTargets,y);
                elapsedSeconds = toc(tstart); % time elapsed for this combination
                numEpochs = tr.num_epochs; % actual no. of epochs trained for
                
            	% data to put into the results table for each combination and fold 
                s = struct('HiddenTransferFcn', hiddenTransferFcn, ...
                        'HiddenLayerSize', mat2str(hiddenLayerSize), ...
                        'NormalizingFcn', normalizeFcn, ...
                        'NumEpochs', numEpochs, ...
                        'Fold', foldIdx, ...
                        'Performance', performance, ...
                        'PercentErrors', percentErrors, ...
                        'TrainPerformance', trainPerformance, ...
                        'ValPerformance', valPerformance, ...
                        'ElapsedSeconds', elapsedSeconds)

                results = [results; struct2table(s, 'AsArray', true)];
                
				% stop training the current configuration if
				% old NN had better validation performance than current NN
                if oldValPerformance < valPerformance
                    break;
                end
                
				% add current NN to array
                nets = [ nets; { net, hiddenTransferFcn, foldIdx, ...
                                performance, percentErrors, ...
                                trainPerformance, valPerformance, ...
                                elapsedSeconds } ];
                
                oldValPerformance = valPerformance;
            end
        end
    end
end

time_to_execute_entire_test = toc(tstart_entire_test)